# ğŸš€ å®Œæ•´è®­ç»ƒæµç¨‹æŒ‡å—

è¿™æ˜¯ä¸€ä¸ªè¯¦ç»†çš„ã€step-by-stepçš„è®­ç»ƒæµç¨‹ï¼Œä»ç¯å¢ƒå‡†å¤‡åˆ°æœ€ç»ˆè®­ç»ƒå®Œæˆã€‚

---

## ğŸ“‹ æ€»è§ˆ

```
é˜¶æ®µ0: ç¯å¢ƒå‡†å¤‡ (1-2å°æ—¶)
   â†“
é˜¶æ®µ1: é¢„è®­ç»ƒåˆ†è§£ç½‘ç»œ (6-8å°æ—¶ï¼Œç‹¬ç«‹è®­ç»ƒ)
   â†“
é˜¶æ®µ2: é›†æˆåˆ°FLUXè®­ç»ƒ (2-3å¤©ï¼Œä¸»è®­ç»ƒ)
   â†“
é˜¶æ®µ3: ç«¯åˆ°ç«¯å¾®è°ƒ (12å°æ—¶ï¼Œå¯é€‰)
```

---

## ğŸ”§ é˜¶æ®µ0: ç¯å¢ƒå‡†å¤‡

### **æ­¥éª¤0.1: åˆ›å»ºPythonç¯å¢ƒ**

```bash
# åˆ›å»ºcondaç¯å¢ƒ
conda create -n physics_ir python=3.10 -y
conda activate physics_ir

# éªŒè¯Pythonç‰ˆæœ¬
python --version  # åº”è¯¥æ˜¾ç¤º Python 3.10.x
```

---

### **æ­¥éª¤0.2: å®‰è£…PyTorch**

```bash
# æ ¹æ®æ‚¨çš„CUDAç‰ˆæœ¬é€‰æ‹©ï¼ˆè¿™é‡Œä»¥CUDA 12.1ä¸ºä¾‹ï¼‰
conda install pytorch torchvision pytorch-cuda=12.1 -c pytorch -c nvidia -y

# éªŒè¯å®‰è£…
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}'); print(f'CUDA version: {torch.version.cuda}')"

# é¢„æœŸè¾“å‡º:
# PyTorch: 2.x.x
# CUDA available: True
# CUDA version: 12.1
```

---

### **æ­¥éª¤0.3: å®‰è£…æ ¸å¿ƒä¾èµ–**

```bash
# Diffusionç›¸å…³
pip install diffusers==0.25.0
pip install transformers==4.35.0
pip install accelerate==0.25.0
pip install peft==0.7.0

# è®­ç»ƒæ¡†æ¶
pip install lightning==2.1.0
pip install prodigyopt==1.0

# æ•°æ®å¤„ç†
pip install datasets==2.16.0
pip install pyarrow==14.0.0
pip install pillow==10.0.0
pip install opencv-python==4.8.0
pip install scikit-learn==1.3.0

# æ—¥å¿—å’Œå¯è§†åŒ–
pip install tensorboard==2.15.0
pip install wandb==0.16.0
pip install matplotlib==3.7.0
pip install seaborn==0.12.0
pip install tqdm

# éªŒè¯å®‰è£…
pip list | grep -E "diffusers|transformers|lightning|datasets"
```

---

### **æ­¥éª¤0.4: HuggingFaceè®¤è¯**

```bash
# ç™»å½•HuggingFaceï¼ˆFLUXæ¨¡å‹éœ€è¦ï¼‰
huggingface-cli login

# è¾“å…¥æ‚¨çš„tokenï¼ˆä» https://huggingface.co/settings/tokens è·å–ï¼‰
# é€‰æ‹© 'y' ä¿å­˜token

# éªŒè¯ç™»å½•
huggingface-cli whoami
```

---

### **æ­¥éª¤0.5: æ•°æ®æ£€æŸ¥**

```bash
# æ£€æŸ¥æ‚¨çš„parquetæ•°æ®é›†
python -c "
from datasets import load_dataset
import sys

# ä¿®æ”¹ä¸ºæ‚¨çš„å®é™…è·¯å¾„
parquet_path = '/root/autodl-tmp/qyt_1/dataset/pid_llvip_dataset.parquet'

try:
    dataset = load_dataset('parquet', data_files=parquet_path)
    print('âœ… æ•°æ®é›†åŠ è½½æˆåŠŸ')
    print(f'Columns: {dataset[\"train\"].column_names}')
    print(f'Total samples: {len(dataset[\"train\"])}')

    # æ£€æŸ¥ç¬¬ä¸€ä¸ªæ ·æœ¬
    sample = dataset['train'][0]
    print(f'\\næ ·æœ¬æ£€æŸ¥:')
    print(f'  src_img: {type(sample[\"src_img\"])}, size: {sample[\"src_img\"].size}')
    print(f'  edited_img: {type(sample[\"edited_img\"])}, size: {sample[\"edited_img\"].size}')
    if 'panoptic_img' in sample:
        print(f'  panoptic_img: {type(sample[\"panoptic_img\"])}, size: {sample[\"panoptic_img\"].size}')
    else:
        print('  âš ï¸ è­¦å‘Š: æ²¡æœ‰panoptic_imgåˆ—')

except Exception as e:
    print(f'âŒ é”™è¯¯: {e}')
    sys.exit(1)
"
```

**é¢„æœŸè¾“å‡º:**
```
âœ… æ•°æ®é›†åŠ è½½æˆåŠŸ
Columns: ['src_img', 'edited_img', 'panoptic_img', 'edited_prompt_list', 'task']
Total samples: 10000

æ ·æœ¬æ£€æŸ¥:
  src_img: <class 'PIL.Image.Image'>, size: (640, 480)
  edited_img: <class 'PIL.Image.Image'>, size: (640, 480)
  panoptic_img: <class 'PIL.Image.Image'>, size: (640, 480)
```

---

### **æ­¥éª¤0.6: åˆ›å»ºæ•°æ®é€‚é…å™¨**

åˆ›å»ºæ–‡ä»¶ `physics_inspired_infrared_generation/utils/parquet_adapter.py`:

```bash
cd physics_inspired_infrared_generation
touch utils/parquet_adapter.py
```

ç„¶åå°†ä»¥ä¸‹ä»£ç å¤åˆ¶åˆ°è¯¥æ–‡ä»¶ï¼š

```python
"""
Parquetæ•°æ®é›†é€‚é…å™¨
å°†æ‚¨çš„parquetæ•°æ®é€‚é…åˆ°åˆ†è§£ç½‘ç»œè®­ç»ƒæ ¼å¼
"""
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import torch

class ParquetToDecompositionDataset(Dataset):
    """
    é€‚é…å™¨: parquet â†’ åˆ†è§£ç½‘ç»œè®­ç»ƒæ ¼å¼
    """
    def __init__(self, parquet_dataset, condition_size=512, use_panoptic=True):
        self.dataset = parquet_dataset
        self.condition_size = condition_size
        self.use_panoptic = use_panoptic

        # RGB transform (ImageNet normalization for ResNet)
        self.rgb_transform = transforms.Compose([
            transforms.Resize((condition_size, condition_size)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                               std=[0.229, 0.224, 0.225])
        ])

        # IR and semantic transform (no normalization)
        self.basic_transform = transforms.Compose([
            transforms.Resize((condition_size, condition_size)),
            transforms.ToTensor()
        ])

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        sample = self.dataset[idx]

        # Load images
        visible_pil = sample['src_img'].convert('RGB')
        infrared_pil = sample['edited_img'].convert('RGB')  # Keep 3 channels

        # Transform
        visible = self.rgb_transform(visible_pil)
        infrared = self.basic_transform(infrared_pil)

        result = {
            'rgb': visible,      # [3, 512, 512] normalized
            'infrared': infrared,  # [3, 512, 512] [0,1]
            'image_id': str(idx)
        }

        # Add panoptic segmentation
        if self.use_panoptic and 'panoptic_img' in sample:
            panoptic_pil = sample['panoptic_img'].convert('RGB')
            panoptic = self.basic_transform(panoptic_pil)
            result['semantic'] = panoptic  # [3, 512, 512]

        return result


def create_dataloaders_from_parquet(
    parquet_path,
    batch_size=8,
    num_workers=4,
    train_split=0.9,
    use_panoptic=True
):
    """
    ä»parquetæ–‡ä»¶åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®åŠ è½½å™¨
    """
    from datasets import load_dataset

    # Load parquet
    dataset = load_dataset('parquet', data_files=parquet_path)
    full_dataset = dataset['train']

    # Split train/val
    split_dataset = full_dataset.train_test_split(
        train_size=train_split,
        seed=42
    )

    # Create datasets
    train_dataset = ParquetToDecompositionDataset(
        split_dataset['train'],
        use_panoptic=use_panoptic
    )
    val_dataset = ParquetToDecompositionDataset(
        split_dataset['test'],
        use_panoptic=use_panoptic
    )

    # Create dataloaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        drop_last=True
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True
    )

    return train_loader, val_loader


# æµ‹è¯•ä»£ç 
if __name__ == '__main__':
    import sys

    # ä¿®æ”¹ä¸ºæ‚¨çš„å®é™…è·¯å¾„
    parquet_path = sys.argv[1] if len(sys.argv) > 1 else './data.parquet'

    print(f"Testing data loading from: {parquet_path}")

    train_loader, val_loader = create_dataloaders_from_parquet(
        parquet_path,
        batch_size=2,
        num_workers=0,
        use_panoptic=True
    )

    print(f"\nâœ… Dataloaders created successfully")
    print(f"Train batches: {len(train_loader)}")
    print(f"Val batches: {len(val_loader)}")

    # Test one batch
    batch = next(iter(train_loader))
    print(f"\nğŸ“¦ Batch contents:")
    print(f"  rgb: {batch['rgb'].shape}, dtype: {batch['rgb'].dtype}")
    print(f"  infrared: {batch['infrared'].shape}, dtype: {batch['infrared'].dtype}")
    if 'semantic' in batch:
        print(f"  semantic: {batch['semantic'].shape}, dtype: {batch['semantic'].dtype}")
    print(f"  image_id: {batch['image_id'][:2]}")

    print("\nâœ… Data loading test passed!")
```

**æµ‹è¯•é€‚é…å™¨:**

```bash
# æµ‹è¯•æ•°æ®åŠ è½½ï¼ˆä¿®æ”¹ä¸ºæ‚¨çš„å®é™…è·¯å¾„ï¼‰
python utils/parquet_adapter.py /root/autodl-tmp/qyt_1/dataset/pid_llvip_dataset.parquet
```

---

## ğŸ¯ é˜¶æ®µ1: é¢„è®­ç»ƒåˆ†è§£ç½‘ç»œ

### **ç›®æ ‡**
ç‹¬ç«‹è®­ç»ƒåˆ†è§£ç½‘ç»œï¼Œå­¦ä¹ ä»RGBæå– intensity, material, context ç‰¹å¾ã€‚

### **æ­¥éª¤1.1: ä¿®æ”¹ä¼ªæ ‡ç­¾ç”Ÿæˆå™¨**

ç¼–è¾‘ `decomposition/pseudo_labels.py`ï¼Œåœ¨æ–‡ä»¶æœ«å°¾æ·»åŠ ï¼š

```python
def generate_pseudo_material_from_panoptic(semantic_img, num_classes=32):
    """
    ä»å…¨æ™¯åˆ†å‰²å›¾ï¼ˆé¢œè‰²åˆ†å—å›¾ï¼‰ç”Ÿæˆææ–™ä¼ªæ ‡ç­¾

    Args:
        semantic_img: [B, 3, H, W] RGBé¢œè‰²åˆ†å—å›¾
        num_classes: ææ–™ç±»åˆ«æ•°

    Returns:
        pseudo_material: [B, H, W] ç±»åˆ«ç´¢å¼• (0åˆ°num_classes-1)
    """
    import numpy as np

    B, _, H, W = semantic_img.shape
    device = semantic_img.device

    pseudo_material = torch.zeros(B, H, W, dtype=torch.long, device=device)

    for i in range(B):
        # è½¬æ¢ä¸ºnumpy [H, W, 3]
        rgb = semantic_img[i].permute(1, 2, 0).cpu().numpy()

        # å°†RGBè½¬æ¢ä¸ºé¢œè‰²ID
        rgb_uint = (rgb * 255).astype(np.uint32)
        color_ids = (rgb_uint[:, :, 0] << 16) | (rgb_uint[:, :, 1] << 8) | rgb_uint[:, :, 2]

        # è·å–å”¯ä¸€é¢œè‰²
        unique_colors, inverse = np.unique(color_ids, return_inverse=True)

        # æ˜ å°„åˆ°num_classes
        class_ids = (np.arange(len(unique_colors)) % num_classes)
        labels = class_ids[inverse].reshape(H, W)

        pseudo_material[i] = torch.from_numpy(labels).long().to(device)

    return pseudo_material


# ä¿®æ”¹PseudoLabelGeneratorç±»ï¼Œåœ¨__init__ä¸­æ·»åŠ å‚æ•°
class PseudoLabelGenerator:
    def __init__(
        self,
        num_material_classes=32,
        context_channels=8,
        material_method='kmeans',
        use_panoptic=True,  # æ–°å¢
        cache_materials=True
    ):
        self.num_material_classes = num_material_classes
        self.context_channels = context_channels
        self.material_method = material_method
        self.use_panoptic = use_panoptic  # æ–°å¢
        self.cache_materials = cache_materials
        self.material_cache = {}

    def __call__(self, rgb_image, infrared_image, semantic_image=None, image_ids=None):
        # Intensity
        pseudo_intensity = generate_pseudo_intensity(infrared_image, normalize=True)

        # Material (ä¼˜å…ˆä½¿ç”¨panoptic)
        if self.use_panoptic and semantic_image is not None:
            pseudo_material = generate_pseudo_material_from_panoptic(
                semantic_image, self.num_material_classes
            )
        else:
            # å›é€€åˆ°èšç±»
            pseudo_material = generate_pseudo_material(
                rgb_image, infrared_image,
                num_classes=self.num_material_classes,
                method=self.material_method
            )

        # Context
        pseudo_context = generate_pseudo_context(
            rgb_image, infrared_image, self.context_channels
        )

        return pseudo_intensity, pseudo_material, pseudo_context
```

---

### **æ­¥éª¤1.2: ä¿®æ”¹é¢„è®­ç»ƒè„šæœ¬**

ç¼–è¾‘ `scripts/pretrain_decomposition.py`ï¼Œåœ¨å¼€å¤´æ·»åŠ å¯¼å…¥ï¼š

```python
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils.parquet_adapter import create_dataloaders_from_parquet
```

ç„¶ååœ¨ `main()` å‡½æ•°ä¸­ä¿®æ”¹æ•°æ®åŠ è½½éƒ¨åˆ†ï¼š

```python
def main(args):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.checkpoint_dir, exist_ok=True)
    os.makedirs(args.log_dir, exist_ok=True)
    os.makedirs(args.vis_dir, exist_ok=True)

    # ===== ä¿®æ”¹ï¼šä½¿ç”¨parquetæ•°æ®åŠ è½½å™¨ =====
    print("Loading datasets from parquet...")
    train_loader, val_loader = create_dataloaders_from_parquet(
        parquet_path=args.train_data,  # parquetæ–‡ä»¶è·¯å¾„
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        train_split=0.9,
        use_panoptic=True  # ä½¿ç”¨å…¨æ™¯åˆ†å‰²å›¾
    )

    print(f"Train batches: {len(train_loader)}")
    print(f"Val batches: {len(val_loader)}")

    # åˆ›å»ºæ¨¡å‹ï¼ˆä¸å˜ï¼‰
    print("Creating model...")
    model = MultiTaskDecompositionNet(
        backbone=args.backbone,
        pretrained=True,
        num_material_classes=args.num_material_classes,
        context_channels=args.context_channels
    ).to(device)

    fusion_module = PhysicsInspiredFusion(
        num_material_classes=args.num_material_classes,
        context_channels=args.context_channels,
        hidden_dim=args.fusion_hidden_dim
    ).to(device)

    # åˆ›å»ºæŸå¤±å‡½æ•°
    loss_fn = MultiTaskPretrainLoss(
        lambda_intensity=args.lambda_intensity,
        lambda_material=args.lambda_material,
        lambda_context=args.lambda_context,
        lambda_fusion=args.lambda_fusion
    ).to(device)

    # ===== ä¿®æ”¹ï¼šåˆ›å»ºä¼ªæ ‡ç­¾ç”Ÿæˆå™¨ï¼Œå¯ç”¨panoptic =====
    pseudo_gen = PseudoLabelGenerator(
        num_material_classes=args.num_material_classes,
        context_channels=args.context_channels,
        material_method=args.material_method,
        use_panoptic=True,  # ä½¿ç”¨å…¨æ™¯åˆ†å‰²å›¾
        cache_materials=True
    )

    # å…¶ä½™ä»£ç ä¸å˜...
```

ç„¶åä¿®æ”¹ `train_one_epoch()` å‡½æ•°ï¼Œæ”¯æŒsemanticè¾“å…¥ï¼š

```python
def train_one_epoch(
    model,
    fusion_module,
    dataloader,
    loss_fn,
    pseudo_gen,
    optimizer,
    device,
    epoch,
    writer=None,
    log_interval=50
):
    """Train for one epoch"""
    model.train()
    fusion_module.train()

    epoch_losses = {
        'total': [],
        'intensity': [],
        'material': [],
        'context': [],
        'fusion': []
    }

    pbar = tqdm(dataloader, desc=f"Epoch {epoch}")

    for batch_idx, batch in enumerate(pbar):
        rgb = batch['rgb'].to(device)
        infrared = batch['infrared'].to(device)
        semantic = batch.get('semantic', None)  # è·å–semanticï¼ˆå¦‚æœæœ‰ï¼‰
        if semantic is not None:
            semantic = semantic.to(device)
        image_ids = batch['image_id']

        # ===== ä¿®æ”¹ï¼šä¼ å…¥semanticç»™ä¼ªæ ‡ç­¾ç”Ÿæˆå™¨ =====
        pseudo_intensity, pseudo_material, pseudo_context = pseudo_gen(
            rgb, infrared, semantic, image_ids
        )

        # Forward passï¼ˆä¸å˜ï¼‰
        pred_intensity, pred_material_logits, pred_context = model(rgb)
        fused_output = fusion_module(pred_intensity, pred_material_logits, pred_context)

        # Compute lossï¼ˆä¸å˜ï¼‰
        loss, loss_dict = loss_fn(
            pred_intensity,
            pred_material_logits,
            pred_context,
            fused_output,
            pseudo_intensity,
            pseudo_material,
            pseudo_context,
            infrared
        )

        # Backward passï¼ˆä¸å˜ï¼‰
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()

        # Log lossesï¼ˆä¸å˜ï¼‰
        for key in epoch_losses.keys():
            epoch_losses[key].append(loss_dict[key])

        pbar.set_postfix({'loss': f"{loss_dict['total']:.4f}"})

        if writer and batch_idx % log_interval == 0:
            global_step = epoch * len(dataloader) + batch_idx
            for key, value in loss_dict.items():
                writer.add_scalar(f'train/{key}_loss', value, global_step)

    avg_losses = {k: sum(v) / len(v) for k, v in epoch_losses.items()}
    return avg_losses
```

åŒæ ·ä¿®æ”¹ `validate()` å‡½æ•°ã€‚

---

### **æ­¥éª¤1.3: åˆ›å»ºé…ç½®æ–‡ä»¶**

åˆ›å»º `configs/pretrain_parquet.yaml`:

```yaml
# æ•°æ®é…ç½®
train_data: "/root/autodl-tmp/qyt_1/dataset/pid_llvip_dataset.parquet"  # ä¿®æ”¹ä¸ºæ‚¨çš„å®é™…è·¯å¾„
val_data: null  # ä¼šè‡ªåŠ¨ä»train_dataåˆ’åˆ†
image_size: 512

# æ¨¡å‹é…ç½®
backbone: "resnet50"
num_material_classes: 32
context_channels: 8
fusion_hidden_dim: 64

# æŸå¤±æƒé‡
lambda_intensity: 1.0
lambda_material: 1.0
lambda_context: 0.5
lambda_fusion: 2.0

# ä¼ªæ ‡ç­¾ç”Ÿæˆ
material_method: "panoptic"  # ä½¿ç”¨å…¨æ™¯åˆ†å‰²å›¾

# è®­ç»ƒé…ç½®
batch_size: 8  # æ ¹æ®GPUæ˜¾å­˜è°ƒæ•´
num_epochs: 100
learning_rate: 0.0001
weight_decay: 0.00001
num_workers: 4

# æ—¥å¿—
checkpoint_dir: "./checkpoints/decomposition"
log_dir: "./logs/decomposition"
vis_dir: "./visualizations/decomposition"
log_interval: 50
val_interval: 1
save_interval: 10
```

---

### **æ­¥éª¤1.4: å¯åŠ¨é˜¶æ®µ1è®­ç»ƒ**

```bash
cd physics_inspired_infrared_generation

# æµ‹è¯•æ•°æ®åŠ è½½ï¼ˆ1åˆ†é’Ÿï¼‰
python scripts/pretrain_decomposition.py \
    --train_data /root/autodl-tmp/qyt_1/dataset/pid_llvip_dataset.parquet \
    --batch_size 2 \
    --num_epochs 1 \
    --num_workers 0

# å¦‚æœæµ‹è¯•é€šè¿‡ï¼Œå¼€å§‹æ­£å¼è®­ç»ƒ
python scripts/pretrain_decomposition.py \
    --config configs/pretrain_parquet.yaml

# æˆ–è€…ç›´æ¥æŒ‡å®šå‚æ•°ï¼ˆå¦‚æœä¸ç”¨é…ç½®æ–‡ä»¶ï¼‰
python scripts/pretrain_decomposition.py \
    --train_data /root/autodl-tmp/qyt_1/dataset/pid_llvip_dataset.parquet \
    --batch_size 8 \
    --num_epochs 100 \
    --backbone resnet50 \
    --num_material_classes 32 \
    --learning_rate 1e-4
```

**é¢„æœŸè¾“å‡º:**
```
Using device: cuda
Loading datasets from parquet...
âœ… Dataloaders created successfully
Train batches: 1125
Val batches: 125
Creating model...
[INFO] MultiTaskDecompositionNet initialized:
  - Backbone: resnet50 (pretrained=True, frozen=False)
  - Projection dim: 64
  - Total parameters: 25,557,032

==================================================
Epoch 1/100
==================================================
Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 1125/1125 [06:23<00:00, loss=2.3456]
Train Loss: 2.3456
Val Loss: 2.1234
Saved best model to ./checkpoints/decomposition/best_model.pth
```

---

### **æ­¥éª¤1.5: ç›‘æ§é˜¶æ®µ1è®­ç»ƒ**

**æ–¹æ³•1: TensorBoard**
```bash
# æ–°å¼€ä¸€ä¸ªç»ˆç«¯
conda activate physics_ir
cd physics_inspired_infrared_generation
tensorboard --logdir ./logs/decomposition --port 6006

# æµè§ˆå™¨æ‰“å¼€ http://localhost:6006
```

**æ–¹æ³•2: æŸ¥çœ‹å¯è§†åŒ–**
```bash
# æ¯ä¸ªepochä¼šç”Ÿæˆå¯è§†åŒ–
ls -lh visualizations/decomposition/

# æŸ¥çœ‹æœ€æ–°çš„å¯è§†åŒ–
# åº”è¯¥çœ‹åˆ°: intensityçªå‡ºäº®åŒºåŸŸ, materialæ˜¾ç¤ºåˆ†å—, contextæ˜¾ç¤ºçº¹ç†
```

**å…³é”®æŒ‡æ ‡:**
- `fusion_loss` åº”è¯¥é€æ¸ä¸‹é™ï¼ˆæœ€é‡è¦ï¼‰
- `intensity_loss` åº”è¯¥ä¸‹é™åˆ° < 0.1
- `material_loss` åº”è¯¥ä¸‹é™åˆ° < 1.0
- å¯è§†åŒ–ä¸­intensityåº”è¯¥ä¸çº¢å¤–å›¾çš„äº®åŒºå¯¹åº”

---

### **æ­¥éª¤1.6: éªŒè¯è®­ç»ƒç»“æœ**

è®­ç»ƒå®Œæˆåï¼ˆçº¦6-8å°æ—¶ï¼‰ï¼ŒéªŒè¯æ¨¡å‹:

```bash
python -c "
import torch
from decomposition.model import MultiTaskDecompositionNet, PhysicsInspiredFusion

# åŠ è½½æœ€ä½³æ¨¡å‹
checkpoint = torch.load('./checkpoints/decomposition/best_model.pth')
print(f'Best epoch: {checkpoint[\"epoch\"]}')
print(f'Best val loss: {checkpoint[\"val_loss\"]:.4f}')

# æµ‹è¯•å‰å‘ä¼ æ’­
model = MultiTaskDecompositionNet(backbone='resnet50', num_material_classes=32)
model.load_state_dict(checkpoint['model_state_dict'])
model.eval().cuda()

rgb = torch.randn(1, 3, 512, 512).cuda()
intensity, material, context = model(rgb)

print(f'\nâœ… æ¨¡å‹åŠ è½½æˆåŠŸ')
print(f'Intensity: {intensity.shape}, range: [{intensity.min():.3f}, {intensity.max():.3f}]')
print(f'Material: {material.shape}, unique classes: {material.argmax(dim=1).unique().numel()}')
print(f'Context: {context.shape}')
"
```

---

## ğŸ”— é˜¶æ®µ2: é›†æˆåˆ°FLUXè®­ç»ƒ

### **ç›®æ ‡**
å°†é¢„è®­ç»ƒçš„åˆ†è§£ç½‘ç»œé›†æˆåˆ°æ‚¨åŸæœ‰çš„FLUXè®­ç»ƒæµç¨‹ä¸­ã€‚

### **æ­¥éª¤2.1: å‡†å¤‡æ–‡ä»¶**

```bash
# å¤åˆ¶åˆ†è§£æ¨¡å—åˆ°åŸé¡¹ç›®
cp -r physics_inspired_infrared_generation/decomposition \
     ICEdit_contrastive/train/src/

# å¤åˆ¶cross-attention
cp physics_inspired_infrared_generation/flux_integration/cross_attention.py \
   ICEdit_contrastive/train/src/decomposition/
```

---

### **æ­¥éª¤2.2: ä¿®æ”¹åŸé¡¹ç›®æ¨¡å‹**

è¿™éƒ¨åˆ†ä»£ç è¾ƒé•¿ï¼Œæˆ‘åˆ›å»ºä¸€ä¸ªpatchæ–‡ä»¶ï¼š

```bash
cd ICEdit_contrastive/train/src/train
```

åœ¨ `model.py` ä¸­æ·»åŠ ï¼ˆåœ¨`__init__`æ–¹æ³•ä¸­ï¼‰:

```python
# åœ¨ __init__ æ–¹æ³•çš„æœ€åï¼Œself.to(device).to(dtype) ä¹‹å‰æ·»åŠ :

        # ===== æ–°å¢ï¼šåˆ†è§£ç½‘ç»œé›†æˆ =====
        use_decomposition = self.model_config.get('use_decomposition_guidance', False)
        self.decomposition_net = None
        self.decomp_encoder = None
        self.decomp_cross_attn = None

        if use_decomposition:
            print('[INFO] Decomposition Guidance ENABLED')

            from ..decomposition.model import MultiTaskDecompositionNet, PhysicsInspiredFusion
            from ..decomposition.cross_attention import DecompositionCrossAttention, DecompositionEncoder

            decomp_config = self.model_config.get('decomposition', {})

            # åˆ›å»ºåˆ†è§£ç½‘ç»œ
            self.decomposition_net = MultiTaskDecompositionNet(
                backbone=decomp_config.get('backbone', 'resnet50'),
                pretrained=False,
                num_material_classes=decomp_config.get('num_material_classes', 32),
                context_channels=decomp_config.get('context_channels', 8)
            )

            # åŠ è½½é¢„è®­ç»ƒæƒé‡
            pretrained_path = decomp_config.get('pretrained_checkpoint', None)
            if pretrained_path and os.path.exists(pretrained_path):
                print(f'[INFO] Loading pretrained decomposition from: {pretrained_path}')
                checkpoint = torch.load(pretrained_path, map_location=device)
                if 'model_state_dict' in checkpoint:
                    self.decomposition_net.load_state_dict(checkpoint['model_state_dict'])
                else:
                    self.decomposition_net.load_state_dict(checkpoint)
                print('[INFO] Decomposition checkpoint loaded')

            # å†»ç»“åˆ†è§£ç½‘ç»œ
            freeze_decomp = decomp_config.get('freeze', True)
            if freeze_decomp:
                for param in self.decomposition_net.parameters():
                    param.requires_grad = False
                self.decomposition_net.eval()
                print('[INFO] Decomposition network is FROZEN')

            # åˆ›å»ºç¼–ç å™¨
            self.decomp_encoder = DecompositionEncoder(
                num_material_classes=decomp_config.get('num_material_classes', 32),
                context_channels=decomp_config.get('context_channels', 8),
                hidden_dim=128,
                output_dim=128
            )

            # åˆ›å»ºäº¤å‰æ³¨æ„åŠ›
            self.decomp_cross_attn = DecompositionCrossAttention(
                image_dim=64,  # FLUX packed latent dimension
                decomp_dim=128,
                num_heads=8,
                dropout=0.1
            )

            print(f'[INFO] Decomposition guidance initialized')

        self.to(device).to(dtype)
```

åœ¨ `configure_optimizers()` æ–¹æ³•ä¸­æ·»åŠ :

```python
def configure_optimizers(self):
    # ... åŸæœ‰ä»£ç  ...

    # æ·»åŠ åˆ†è§£æ¨¡å—å‚æ•°
    if self.decomp_encoder is not None:
        self.trainable_params.extend(list(self.decomp_encoder.parameters()))
        print(f'[INFO] Added decomp_encoder parameters')

    if self.decomp_cross_attn is not None:
        self.trainable_params.extend(list(self.decomp_cross_attn.parameters()))
        print(f'[INFO] Added decomp_cross_attn parameters')

    # ... åˆ›å»ºä¼˜åŒ–å™¨çš„ä»£ç  ...
```

åœ¨ `step()` æ–¹æ³•ä¸­æ·»åŠ ï¼ˆåœ¨semantic processingä¹‹åï¼‰:

```python
def step(self, batch):
    # ... åŸæœ‰çš„è¯­ä¹‰å¤„ç†ä»£ç  ...

    # ===== æ–°å¢ï¼šåˆ†è§£ç½‘ç»œå¤„ç† =====
    decomp_features = None
    use_decomposition = (
        self.decomposition_net is not None and
        self.model_config.get('use_decomposition_guidance', False)
    )

    if use_decomposition and hasattr(self, 'visible_img'):  # visible_imgåœ¨è¯­ä¹‰å¤„ç†æ—¶å·²å®šä¹‰
        from torchvision import transforms

        # ImageNet normalization for decomposition network
        normalize = transforms.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        )
        visible_normalized = normalize(visible_img.float())

        # Forward through decomposition network
        with torch.no_grad() if self.model_config.get('decomposition', {}).get('freeze', True) else torch.enable_grad():
            intensity, material_logits, context = self.decomposition_net(visible_normalized)

        # Encode decomposition features
        decomp_features = self.decomp_encoder(intensity, material_logits, context)

        # Upsample to 2048 sequence length (to match x_t)
        if decomp_features.shape[1] != 2048:
            import torch.nn.functional as F
            decomp_features = F.interpolate(
                decomp_features.transpose(1, 2),
                size=2048,
                mode='linear',
                align_corners=False
            ).transpose(1, 2)

    # ... å‡†å¤‡x_tç­‰ï¼ˆåŸæœ‰ä»£ç ï¼‰...

    # ===== åº”ç”¨äº¤å‰æ³¨æ„åŠ› =====
    # å…ˆåº”ç”¨semantic guidanceï¼ˆå¦‚æœæœ‰ï¼‰
    if self.semantic_cross_attn is not None and semantic_tokens is not None:
        x_t = self.semantic_cross_attn(x_t, semantic_tokens)

    # å†åº”ç”¨decomposition guidanceï¼ˆå¦‚æœæœ‰ï¼‰
    if self.decomp_cross_attn is not None and decomp_features is not None:
        x_t = self.decomp_cross_attn(x_t, decomp_features)

    # é‡æ–°ç»„è£…hidden_states
    hidden_states = torch.cat((x_t, x_cond), dim=2)

    # ... FLUX transformer forwardï¼ˆåŸæœ‰ä»£ç ï¼‰...
```

---

### **æ­¥éª¤2.3: åˆ›å»ºé…ç½®æ–‡ä»¶**

åˆ›å»º `ICEdit_contrastive/train/train/config/vis2ir_decomposition.yaml`:

```yaml
flux_path: "/root/autodl-tmp/qyt/hfd_model"
dtype: "bfloat16"

model:
  union_cond_attn: true
  add_cond_attn: false
  latent_lora: false
  use_sep: false

  # Semantic conditioning
  use_semantic_conditioning: true
  semantic_mode: "cross_attention"
  semantic_num_layers: 1
  semantic_num_heads: 8

  # ===== Decomposition guidance (æ–°å¢) =====
  use_decomposition_guidance: true
  decomposition:
    backbone: "resnet50"
    num_material_classes: 32
    context_channels: 8
    # é‡è¦ï¼šä¿®æ”¹ä¸ºé˜¶æ®µ1è®­ç»ƒçš„æƒé‡è·¯å¾„
    pretrained_checkpoint: "/path/to/physics_inspired_infrared_generation/checkpoints/decomposition/best_model.pth"
    freeze: true  # å†»ç»“åˆ†è§£ç½‘ç»œ

train:
  batch_size: 4
  accumulate_grad_batches: 1
  dataloader_workers: 4
  save_interval: 1000
  sample_interval: 1000
  max_steps: -1
  gradient_checkpointing: true
  save_path: "runs"

  condition_type: "vis2ir_decomposition"
  dataset:
    type: "vis2ir_semantic"
    path: "/root/autodl-tmp/qyt_1/dataset/pid_llvip_dataset.parquet"
    condition_size: 512
    target_size: 512
    image_size: 512
    padding: 8
    drop_text_prob: 0.1
    drop_image_prob: 0.0
    include_semantic: true
    semantic_column: panoptic_img

  wandb:
    project: "Vis2IR-Decomposition"

  lora_config:
    r: 32
    lora_alpha: 32
    init_lora_weights: "gaussian"
    target_modules: "(.*x_embedder|.*(?<!single_)transformer_blocks\\.[0-9]+\\.norm1\\.linear|.*(?<!single_)transformer_blocks\\.[0-9]+\\.attn\\.to_k|.*(?<!single_)transformer_blocks\\.[0-9]+\\.attn\\.to_q|.*(?<!single_)transformer_blocks\\.[0-9]+\\.attn\\.to_v|.*(?<!single_)transformer_blocks\\.[0-9]+\\.attn\\.to_out\\.0|.*(?<!single_)transformer_blocks\\.[0-9]+\\.ff\\.net\\.2|.*single_transformer_blocks\\.[0-9]+\\.norm\\.linear|.*single_transformer_blocks\\.[0-9]+\\.proj_mlp|.*single_transformer_blocks\\.[0-9]+\\.proj_out|.*single_transformer_blocks\\.[0-9]+\\.attn.to_k|.*single_transformer_blocks\\.[0-9]+\\.attn.to_q|.*single_transformer_blocks\\.[0-9]+\\.attn.to_v|.*single_transformer_blocks\\.[0-9]+\\.attn.to_out)"

  optimizer:
    type: "Prodigy"
    params:
      lr: 1
      use_bias_correction: true
      safeguard_warmup: true
      weight_decay: 0.01
```

---

### **æ­¥éª¤2.4: å¯åŠ¨é˜¶æ®µ2è®­ç»ƒ**

```bash
cd ICEdit_contrastive/train

# é‡è¦ï¼šä¿®æ”¹é…ç½®æ–‡ä»¶ä¸­çš„checkpointè·¯å¾„
# ç¼–è¾‘ train/config/vis2ir_decomposition.yaml
# å°† pretrained_checkpoint æ”¹ä¸ºå®é™…è·¯å¾„

# å¯åŠ¨è®­ç»ƒ
python train/train.py \
    --config train/config/vis2ir_decomposition.yaml \
    --devices 0,1  # ä½¿ç”¨çš„GPU ID
```

**é¢„æœŸè¾“å‡º:**
```
[INFO] Decomposition Guidance ENABLED
[INFO] Loading pretrained decomposition from: /path/to/best_model.pth
[INFO] Decomposition checkpoint loaded
[INFO] Decomposition network is FROZEN
[INFO] Decomposition guidance initialized
[INFO] Added decomp_encoder parameters
[INFO] Added decomp_cross_attn parameters
Trainable parameters: 45,123,456

Starting training...
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [2:15:30<00:00, loss=0.0234]
```

---

### **æ­¥éª¤2.5: ç›‘æ§é˜¶æ®µ2è®­ç»ƒ**

```bash
# TensorBoard
tensorboard --logdir runs/ --port 6007

# WandB (å¦‚æœé…ç½®äº†)
# è®¿é—® https://wandb.ai/your-project

# æŸ¥çœ‹ç”Ÿæˆæ ·æœ¬
ls runs/*/samples/
```

---

## ğŸ“ é˜¶æ®µ3: ç«¯åˆ°ç«¯å¾®è°ƒï¼ˆå¯é€‰ï¼‰

### **æ­¥éª¤3.1: ä¿®æ”¹é…ç½®**

```yaml
# vis2ir_decomposition.yaml
model:
  decomposition:
    freeze: false  # è§£å†»åˆ†è§£ç½‘ç»œ

train:
  optimizer:
    params:
      lr: 0.5  # é™ä½å­¦ä¹ ç‡
```

### **æ­¥éª¤3.2: ä»checkpointç»§ç»­è®­ç»ƒ**

```bash
python train/train.py \
    --config train/config/vis2ir_decomposition.yaml \
    --resume_from_checkpoint runs/your_checkpoint.ckpt \
    --devices 0,1
```

---

## ğŸ“Š å®Œæ•´æ—¶é—´çº¿

| é˜¶æ®µ | æ—¶é—´ | GPU | è¾“å‡º |
|------|------|-----|------|
| ç¯å¢ƒå‡†å¤‡ | 1-2å°æ—¶ | - | ä¾èµ–å®‰è£…å®Œæˆ |
| é˜¶æ®µ1 | 6-8å°æ—¶ | 1x3090 | best_model.pth |
| é˜¶æ®µ2 | 2-3å¤© | 2xA100 | FLUX LoRA weights |
| é˜¶æ®µ3 | 12å°æ—¶ | 2xA100 | å¾®è°ƒæƒé‡ |

---

## âœ… æ£€æŸ¥æ¸…å•

å¯åŠ¨å‰ç¡®è®¤:
- [ ] Condaç¯å¢ƒåˆ›å»ºå¹¶æ¿€æ´»
- [ ] PyTorchå’ŒCUDAæ­£ç¡®å®‰è£…
- [ ] HuggingFaceå·²ç™»å½•
- [ ] Parquetæ•°æ®é›†å¯è®¿é—®
- [ ] æ•°æ®åŒ…å« panoptic_img åˆ—
- [ ] GPUæ˜¾å­˜å……è¶³ï¼ˆé˜¶æ®µ1: 24GB, é˜¶æ®µ2: 40GB+ï¼‰
- [ ] ç£ç›˜ç©ºé—´è¶³å¤Ÿï¼ˆè‡³å°‘100GBï¼‰

---

éœ€è¦æˆ‘è¡¥å……ä»»ä½•éƒ¨åˆ†å—ï¼Ÿ
