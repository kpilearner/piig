"""
é˜¶æ®µ1: åˆ†è§£ç½‘ç»œé¢„è®­ç»ƒ

è®­ç»ƒå¤šä»»åŠ¡åˆ†è§£ç½‘ç»œå­¦ä¹ ç‰©ç†å¯å‘çš„è¡¨ç¤ºï¼š
- Intensity: ç±»ä¼¼æ¸©åº¦çš„äº®åº¦åˆ†å¸ƒ
- Material: ç±»ä¼¼å‘å°„ç‡çš„ææ–™ç±»åˆ«ï¼ˆä»panoptic_imgç”Ÿæˆï¼‰
- Context: ç¯å¢ƒç‰¹å¾

ä½¿ç”¨ä¸ICEdit_contrastiveå®Œå…¨ç›¸åŒçš„æ•°æ®æ ¼å¼ï¼ˆparquetï¼‰
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
import os
import sys
from pathlib import Path
from tqdm import tqdm
import argparse
import yaml
import numpy as np

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from decomposition.model import MultiTaskDecompositionNet, PhysicsInspiredFusion
from decomposition.losses import MultiTaskPretrainLoss
from decomposition.pseudo_labels import (
    generate_pseudo_intensity,
    generate_pseudo_material_from_panoptic,
    generate_pseudo_context
)
from utils.parquet_dataloader import create_dataloaders


class DecompositionTrainer:
    """åˆ†è§£ç½‘ç»œè®­ç»ƒå™¨"""

    def __init__(self, config):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # åˆ›å»ºä¿å­˜ç›®å½•
        self.save_dir = Path(config['save_dir'])
        self.save_dir.mkdir(parents=True, exist_ok=True)

        self.vis_dir = self.save_dir / 'visualizations'
        self.vis_dir.mkdir(exist_ok=True)

        # TensorBoard
        self.writer = SummaryWriter(log_dir=str(self.save_dir / 'logs'))

        # åˆ›å»ºæ¨¡å‹
        print("ğŸ—ï¸  åˆ›å»ºåˆ†è§£ç½‘ç»œ...")
        self.model = MultiTaskDecompositionNet(
            backbone=config['model']['backbone'],
            pretrained=True,
            num_material_classes=config['model']['num_material_classes'],
            context_channels=config['model']['context_channels'],
        ).to(self.device)

        self.fusion_module = PhysicsInspiredFusion(
            num_material_classes=config['model']['num_material_classes'],
            context_channels=config['model']['context_channels'],
            hidden_dim=64,
        ).to(self.device)

        print(f"   å‚æ•°é‡: {sum(p.numel() for p in self.model.parameters()) / 1e6:.2f}M")

        # åˆ›å»ºæŸå¤±å‡½æ•°
        self.criterion = MultiTaskPretrainLoss(
            lambda_intensity=config['loss']['lambda_intensity'],
            lambda_material=config['loss']['lambda_material'],
            lambda_context=config['loss']['lambda_context'],
            lambda_fusion=config['loss']['lambda_fusion'],
        )

        # åˆ›å»ºä¼˜åŒ–å™¨ï¼ˆåŒ…å«modelå’Œfusion_moduleï¼‰
        self.optimizer = optim.AdamW(
            list(self.model.parameters()) + list(self.fusion_module.parameters()),
            lr=config['training']['learning_rate'],
            weight_decay=config['training']['weight_decay'],
        )

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=config['training']['num_epochs'],
            eta_min=config['training']['learning_rate'] * 0.01,
        )

        # è®­ç»ƒçŠ¶æ€
        self.epoch = 0
        self.global_step = 0
        self.best_val_loss = float('inf')

    def train_epoch(self, train_loader):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        epoch_losses = {
            'total': 0,
            'intensity': 0,
            'material': 0,
            'context': 0,
            'fusion': 0,
        }

        pbar = tqdm(train_loader, desc=f"Epoch {self.epoch}")
        for batch_idx, batch in enumerate(pbar):
            # ç§»åŠ¨æ•°æ®åˆ°GPU
            rgb = batch['rgb'].to(self.device)
            infrared = batch['infrared'].to(self.device)
            semantic = batch['semantic'].to(self.device)

            # ç”Ÿæˆä¼ªæ ‡ç­¾
            with torch.no_grad():
                # Intensity: ä»çº¢å¤–å›¾åƒç”Ÿæˆ
                ir_gray = infrared.mean(dim=1, keepdim=True)  # [B, 1, H, W]
                pseudo_intensity = generate_pseudo_intensity(ir_gray, normalize=True)

                # Material: ä»panopticåˆ†å‰²å›¾ç”Ÿæˆï¼ˆæ¨èæ–¹æ³•ï¼‰
                pseudo_material = generate_pseudo_material_from_panoptic(
                    semantic,
                    num_classes=self.config['model']['num_material_classes']
                )

                # Context: ä»RGB-IRå·®å¼‚ç”Ÿæˆ
                pseudo_context = generate_pseudo_context(rgb, ir_gray)

                # ç›®æ ‡çº¢å¤–å›¾ï¼ˆç”¨äºfusioné‡å»ºï¼‰
                target_ir = ir_gray

            # å‰å‘ä¼ æ’­
            pred_intensity, pred_material_logits, pred_context = self.model(rgb)

            # Physics-inspired fusion
            fused_output = self.fusion_module(pred_intensity, pred_material_logits, pred_context)

            # è®¡ç®—æŸå¤±
            loss, losses = self.criterion(
                pred_intensity,
                pred_material_logits,
                pred_context,
                fused_output,
                pseudo_intensity,
                pseudo_material,
                pseudo_context,
                target_ir
            )

            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()

            # æ¢¯åº¦è£å‰ªï¼ˆåŒ…å«modelå’Œfusion_moduleï¼‰
            torch.nn.utils.clip_grad_norm_(
                list(self.model.parameters()) + list(self.fusion_module.parameters()),
                self.config['training']['grad_clip']
            )

            self.optimizer.step()

            # è®°å½•æŸå¤±
            for key in epoch_losses.keys():
                epoch_losses[key] += losses[key]

            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'loss': f"{losses['total']:.4f}",
                'fusion': f"{losses['fusion']:.4f}",
            })

            # TensorBoardè®°å½•
            if self.global_step % 10 == 0:
                for key, value in losses.items():
                    self.writer.add_scalar(f'train/{key}_loss', value, self.global_step)
                self.writer.add_scalar('train/lr', self.optimizer.param_groups[0]['lr'], self.global_step)

            self.global_step += 1

        # è®¡ç®—å¹³å‡æŸå¤±
        for key in epoch_losses.keys():
            epoch_losses[key] /= len(train_loader)

        return epoch_losses

    @torch.no_grad()
    def validate(self, val_loader):
        """éªŒè¯"""
        self.model.eval()
        self.fusion_module.eval()
        val_losses = {
            'total': 0,
            'intensity': 0,
            'material': 0,
            'context': 0,
            'fusion': 0,
        }

        for batch in tqdm(val_loader, desc="Validation"):
            rgb = batch['rgb'].to(self.device)
            infrared = batch['infrared'].to(self.device)
            semantic = batch['semantic'].to(self.device)

            # ç”Ÿæˆä¼ªæ ‡ç­¾
            ir_gray = infrared.mean(dim=1, keepdim=True)
            pseudo_intensity = generate_pseudo_intensity(ir_gray, normalize=True)
            pseudo_material = generate_pseudo_material_from_panoptic(
                semantic,
                num_classes=self.config['model']['num_material_classes']
            )
            pseudo_context = generate_pseudo_context(rgb, ir_gray)
            target_ir = ir_gray

            # å‰å‘ä¼ æ’­
            pred_intensity, pred_material_logits, pred_context = self.model(rgb)
            fused_output = self.fusion_module(pred_intensity, pred_material_logits, pred_context)

            # è®¡ç®—æŸå¤±
            loss, losses = self.criterion(
                pred_intensity,
                pred_material_logits,
                pred_context,
                fused_output,
                pseudo_intensity,
                pseudo_material,
                pseudo_context,
                target_ir
            )

            # ç´¯ç§¯æŸå¤±
            for key in val_losses.keys():
                val_losses[key] += losses[key]

        # è®¡ç®—å¹³å‡æŸå¤±
        for key in val_losses.keys():
            val_losses[key] /= len(val_loader)

        return val_losses

    def save_checkpoint(self, filename='checkpoint.pth', is_best=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': self.epoch,
            'global_step': self.global_step,
            'model_state_dict': self.model.state_dict(),
            'fusion_state_dict': self.fusion_module.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_val_loss': self.best_val_loss,
            'config': self.config,
        }

        filepath = self.save_dir / filename
        torch.save(checkpoint, filepath)
        print(f"   ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {filepath}")

        if is_best:
            best_path = self.save_dir / 'best_model.pth'
            torch.save(checkpoint, best_path)
            print(f"   â­ æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {best_path}")

    def train(self, train_loader, val_loader):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print("\n" + "=" * 70)
        print("å¼€å§‹è®­ç»ƒåˆ†è§£ç½‘ç»œ")
        print("=" * 70)

        num_epochs = self.config['training']['num_epochs']

        for epoch in range(num_epochs):
            self.epoch = epoch

            # è®­ç»ƒ
            train_losses = self.train_epoch(train_loader)

            # éªŒè¯
            val_losses = self.validate(val_loader)

            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step()

            # æ‰“å°ç»“æœ
            print(f"\nEpoch {epoch}/{num_epochs}")
            print(f"  Train Loss: {train_losses['total']:.4f} "
                  f"(Int: {train_losses['intensity']:.4f}, "
                  f"Mat: {train_losses['material']:.4f}, "
                  f"Ctx: {train_losses['context']:.4f}, "
                  f"Fus: {train_losses['fusion']:.4f})")
            print(f"  Val Loss:   {val_losses['total']:.4f} "
                  f"(Int: {val_losses['intensity']:.4f}, "
                  f"Mat: {val_losses['material']:.4f}, "
                  f"Ctx: {val_losses['context']:.4f}, "
                  f"Fus: {val_losses['fusion']:.4f})")

            # TensorBoardè®°å½•
            for key, value in val_losses.items():
                self.writer.add_scalar(f'val/{key}_loss', value, epoch)

            # ä¿å­˜æ£€æŸ¥ç‚¹
            is_best = val_losses['total'] < self.best_val_loss
            if is_best:
                self.best_val_loss = val_losses['total']

            if (epoch + 1) % self.config['training']['save_interval'] == 0:
                self.save_checkpoint(f'epoch_{epoch}.pth', is_best=is_best)

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self.save_checkpoint('final_model.pth')

        print("\n" + "=" * 70)
        print("âœ… è®­ç»ƒå®Œæˆ!")
        print(f"   æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.4f}")
        print(f"   æ¨¡å‹ä¿å­˜è·¯å¾„: {self.save_dir}")
        print("=" * 70)

        self.writer.close()


def main():
    parser = argparse.ArgumentParser(description='é¢„è®­ç»ƒåˆ†è§£ç½‘ç»œ')
    parser.add_argument('--config', type=str, required=True,
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--parquet', type=str, default=None,
                       help='Parquetæ–‡ä»¶è·¯å¾„ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰')
    parser.add_argument('--batch_size', type=int, default=None,
                       help='æ‰¹æ¬¡å¤§å°ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰')
    parser.add_argument('--num_epochs', type=int, default=None,
                       help='è®­ç»ƒè½®æ•°ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰')

    args = parser.parse_args()

    # åŠ è½½é…ç½®
    with open(args.config, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)

    # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–
    if args.parquet:
        config['data']['parquet_path'] = args.parquet
    if args.batch_size:
        config['data']['batch_size'] = args.batch_size
    if args.num_epochs:
        config['training']['num_epochs'] = args.num_epochs

    print("=" * 70)
    print("é…ç½®ä¿¡æ¯:")
    print("=" * 70)
    print(f"  æ•°æ®è·¯å¾„: {config['data']['parquet_path']}")
    print(f"  æ‰¹æ¬¡å¤§å°: {config['data']['batch_size']}")
    print(f"  è®­ç»ƒè½®æ•°: {config['training']['num_epochs']}")
    print(f"  å­¦ä¹ ç‡: {config['training']['learning_rate']}")
    print(f"  ä¿å­˜è·¯å¾„: {config['save_dir']}")
    print("=" * 70)

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader, val_loader = create_dataloaders(
        parquet_path=config['data']['parquet_path'],
        split_file=config['data']['split_file'],
        batch_size=config['data']['batch_size'],
        num_workers=config['data']['num_workers'],
        image_size=config['data']['image_size'],
        normalize=True,
    )

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = DecompositionTrainer(config)

    # å¼€å§‹è®­ç»ƒ
    trainer.train(train_loader, val_loader)


if __name__ == '__main__':
    main()
