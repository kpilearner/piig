"""
å›ºå®šæ•°æ®é›†åˆ’åˆ†å·¥å…·

ç¡®ä¿é˜¶æ®µ1é¢„è®­ç»ƒå’Œé˜¶æ®µ2 FLUXè®­ç»ƒä½¿ç”¨å®Œå…¨ç›¸åŒçš„è®­ç»ƒ/éªŒè¯é›†åˆ’åˆ†ï¼Œ
é¿å…æ•°æ®æ³„æ¼ã€‚

ä¸ICEdit_contrastiveä¿æŒä¸€è‡´çš„ç¼“å­˜ç­–ç•¥ï¼š
- ä½¿ç”¨ç¯å¢ƒå˜é‡ HF_DATASETS_CACHE æ§åˆ¶datasetsç¼“å­˜ä½ç½®
- ä½¿ç”¨ç¯å¢ƒå˜é‡ HUGGINGFACE_HUB_CACHE æ§åˆ¶æ¨¡å‹ç¼“å­˜ä½ç½®
"""

import json
import os
from pathlib import Path
import numpy as np
import pyarrow.parquet as pq
from datasets import load_dataset

# è¯»å–ç¯å¢ƒå˜é‡ä¸­çš„ç¼“å­˜è·¯å¾„ï¼ˆä¸ICEditä¿æŒä¸€è‡´ï¼‰
# å¦‚æœæ²¡æœ‰è®¾ç½®ï¼Œä½¿ç”¨é»˜è®¤ä½ç½®
DATASETS_CACHE = os.environ.get('HF_DATASETS_CACHE', None)


def create_fixed_split(
    parquet_path,
    split_ratio=0.9,
    seed=42,
    output_dir='./data',
    force_recreate=False
):
    """
    åˆ›å»ºå¹¶ä¿å­˜å›ºå®šçš„æ•°æ®é›†åˆ’åˆ†ç´¢å¼•

    Args:
        parquet_path: parquetæ–‡ä»¶è·¯å¾„
        split_ratio: è®­ç»ƒé›†æ¯”ä¾‹ï¼ˆé»˜è®¤0.9 = 90%è®­ç»ƒï¼Œ10%éªŒè¯ï¼‰
        seed: éšæœºç§å­ï¼ˆå›ºå®šä¸º42ç¡®ä¿å¯å¤ç°ï¼‰
        output_dir: è¾“å‡ºç›®å½•
        force_recreate: æ˜¯å¦å¼ºåˆ¶é‡æ–°åˆ›å»ºï¼ˆé»˜è®¤Falseï¼Œå¦‚æœå·²å­˜åœ¨åˆ™è·³è¿‡ï¼‰

    Returns:
        split_info: dictåŒ…å«åˆ’åˆ†ä¿¡æ¯
    """
    os.makedirs(output_dir, exist_ok=True)

    # åˆ’åˆ†ä¿¡æ¯æ–‡ä»¶è·¯å¾„
    split_file = os.path.join(output_dir, 'dataset_split.json')

    # å¦‚æœå·²å­˜åœ¨ä¸”ä¸å¼ºåˆ¶é‡æ–°åˆ›å»ºï¼Œç›´æ¥åŠ è½½
    if os.path.exists(split_file) and not force_recreate:
        print(f"âœ… ä½¿ç”¨å·²å­˜åœ¨çš„åˆ’åˆ†æ–‡ä»¶: {split_file}")
        with open(split_file, 'r') as f:
            split_info = json.load(f)

        # éªŒè¯
        print(f"   è®­ç»ƒé›†å¤§å°: {len(split_info['train_indices'])}")
        print(f"   éªŒè¯é›†å¤§å°: {len(split_info['val_indices'])}")
        print(f"   æ€»è®¡: {split_info['total_size']}")
        print(f"   åˆ’åˆ†æ¯”ä¾‹: {split_info['split_ratio']}")
        print(f"   éšæœºç§å­: {split_info['seed']}")

        return split_info

    # ä½¿ç”¨PyArrowç›´æ¥è¯»å–å…ƒæ•°æ®ï¼Œé¿å…åŠ è½½æ‰€æœ‰æ•°æ®åˆ°å†…å­˜
    print(f"ğŸ“‚ è¯»å–æ•°æ®é›†å…ƒæ•°æ®: {parquet_path}")
    if DATASETS_CACHE:
        print(f"   ç¼“å­˜è·¯å¾„: {DATASETS_CACHE}")

    # åªè¯»å–å…ƒæ•°æ®è·å–è¡Œæ•°ï¼Œä¸åŠ è½½ä»»ä½•å®é™…æ•°æ®
    parquet_file = pq.ParquetFile(parquet_path)
    total_size = parquet_file.metadata.num_rows
    print(f"   æ€»æ ·æœ¬æ•°: {total_size}")

    # è®¡ç®—åˆ’åˆ†ç‚¹
    train_size = int(total_size * split_ratio)
    val_size = total_size - train_size

    print(f"ğŸ”€ åˆ’åˆ†æ•°æ®é›† (ratio={split_ratio}, seed={seed})...")
    print(f"   è®­ç»ƒé›†: {train_size} æ ·æœ¬")
    print(f"   éªŒè¯é›†: {val_size} æ ·æœ¬")

    # ç”Ÿæˆç´¢å¼•ï¼ˆä½¿ç”¨å›ºå®šç§å­ï¼‰
    np.random.seed(seed)
    all_indices = np.arange(total_size)
    np.random.shuffle(all_indices)

    train_indices = sorted(all_indices[:train_size].tolist())
    val_indices = sorted(all_indices[train_size:].tolist())

    # ä¿å­˜åˆ’åˆ†ä¿¡æ¯
    split_info = {
        'parquet_path': str(Path(parquet_path).absolute()),
        'total_size': total_size,
        'train_size': len(train_indices),
        'val_size': len(val_indices),
        'train_indices': train_indices,
        'val_indices': val_indices,
        'split_ratio': split_ratio,
        'seed': seed,
        'created_at': str(Path(split_file).stat().st_mtime if os.path.exists(split_file) else 'new')
    }

    with open(split_file, 'w') as f:
        json.dump(split_info, f, indent=2)

    print(f"âœ… åˆ’åˆ†ä¿¡æ¯å·²ä¿å­˜åˆ°: {split_file}")
    print(f"   è®­ç»ƒé›†: {split_info['train_size']} æ ·æœ¬")
    print(f"   éªŒè¯é›†: {split_info['val_size']} æ ·æœ¬")

    return split_info


def load_split_dataset(
    parquet_path,
    split_file='./data/dataset_split.json',
    create_if_not_exists=True
):
    """
    åŠ è½½ä½¿ç”¨å›ºå®šåˆ’åˆ†çš„æ•°æ®é›†

    Args:
        parquet_path: parquetæ–‡ä»¶è·¯å¾„
        split_file: åˆ’åˆ†ä¿¡æ¯æ–‡ä»¶è·¯å¾„
        create_if_not_exists: å¦‚æœåˆ’åˆ†æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ˜¯å¦è‡ªåŠ¨åˆ›å»º

    Returns:
        train_dataset, val_dataset: è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†
    """
    # æ£€æŸ¥åˆ’åˆ†æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(split_file):
        if create_if_not_exists:
            print(f"âš ï¸  åˆ’åˆ†æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè‡ªåŠ¨åˆ›å»º...")
            split_info = create_fixed_split(
                parquet_path,
                output_dir=os.path.dirname(split_file) or './data'
            )
        else:
            raise FileNotFoundError(
                f"åˆ’åˆ†æ–‡ä»¶ä¸å­˜åœ¨: {split_file}\n"
                f"è¯·å…ˆè¿è¡Œ create_fixed_split() åˆ›å»ºåˆ’åˆ†"
            )
    else:
        # åŠ è½½åˆ’åˆ†ä¿¡æ¯
        with open(split_file, 'r') as f:
            split_info = json.load(f)

        print(f"âœ… åŠ è½½åˆ’åˆ†ä¿¡æ¯: {split_file}")

    # åŠ è½½å®Œæ•´æ•°æ®é›†ï¼ˆä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„ç¼“å­˜è·¯å¾„ï¼‰
    print(f"ğŸ“‚ åŠ è½½æ•°æ®é›†: {parquet_path}")
    dataset = load_dataset(
        'parquet',
        data_files=parquet_path,
        cache_dir=DATASETS_CACHE  # ä½¿ç”¨ä¸ICEditç›¸åŒçš„ç¼“å­˜ç­–ç•¥
    )
    full_dataset = dataset['train']

    # éªŒè¯æ•°æ®é›†å¤§å°
    if len(full_dataset) != split_info['total_size']:
        print(f"âš ï¸  è­¦å‘Š: æ•°æ®é›†å¤§å°ä¸åŒ¹é…!")
        print(f"   æœŸæœ›: {split_info['total_size']}")
        print(f"   å®é™…: {len(full_dataset)}")
        print(f"   å»ºè®®é‡æ–°åˆ›å»ºåˆ’åˆ†æ–‡ä»¶")

    # ä½¿ç”¨ä¿å­˜çš„ç´¢å¼•è¿›è¡Œåˆ’åˆ†
    train_indices = split_info['train_indices']
    val_indices = split_info['val_indices']

    train_dataset = full_dataset.select(train_indices)
    val_dataset = full_dataset.select(val_indices)

    print(f"ğŸ“Š æ•°æ®é›†åŠ è½½å®Œæˆ:")
    print(f"   è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
    print(f"   éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")

    return train_dataset, val_dataset


def verify_split_consistency(split_file='./data/dataset_split.json'):
    """
    éªŒè¯æ•°æ®é›†åˆ’åˆ†çš„ä¸€è‡´æ€§

    æ£€æŸ¥JSONæ–‡ä»¶çš„å®Œæ•´æ€§å’Œç´¢å¼•çš„åˆç†æ€§
    """
    print("ğŸ” éªŒè¯æ•°æ®é›†åˆ’åˆ†ä¸€è‡´æ€§...")

    if not os.path.exists(split_file):
        print(f"âŒ åˆ’åˆ†æ–‡ä»¶ä¸å­˜åœ¨: {split_file}")
        return False

    with open(split_file, 'r') as f:
        split_info = json.load(f)

    # éªŒè¯å¿…éœ€å­—æ®µ
    required_fields = ['parquet_path', 'total_size', 'train_size', 'val_size',
                       'train_indices', 'val_indices', 'split_ratio', 'seed']
    for field in required_fields:
        if field not in split_info:
            print(f"âŒ ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
            return False

    # éªŒè¯æ•°æ®ä¸€è‡´æ€§
    train_indices = split_info['train_indices']
    val_indices = split_info['val_indices']

    if len(train_indices) != split_info['train_size']:
        print(f"âŒ è®­ç»ƒé›†ç´¢å¼•æ•°é‡ä¸åŒ¹é…: {len(train_indices)} vs {split_info['train_size']}")
        return False

    if len(val_indices) != split_info['val_size']:
        print(f"âŒ éªŒè¯é›†ç´¢å¼•æ•°é‡ä¸åŒ¹é…: {len(val_indices)} vs {split_info['val_size']}")
        return False

    if len(train_indices) + len(val_indices) != split_info['total_size']:
        print(f"âŒ æ€»æ ·æœ¬æ•°ä¸åŒ¹é…")
        return False

    # éªŒè¯ç´¢å¼•æ— é‡å 
    train_set = set(train_indices)
    val_set = set(val_indices)
    if train_set & val_set:
        print(f"âŒ è®­ç»ƒé›†å’ŒéªŒè¯é›†æœ‰é‡å ")
        return False

    # éªŒè¯ç´¢å¼•èŒƒå›´
    all_indices = train_set | val_set
    if min(all_indices) < 0 or max(all_indices) >= split_info['total_size']:
        print(f"âŒ ç´¢å¼•è¶…å‡ºèŒƒå›´")
        return False

    print("âœ… æ•°æ®é›†åˆ’åˆ†ä¸€è‡´æ€§éªŒè¯é€šè¿‡!")
    print(f"   è®­ç»ƒé›†: {split_info['train_size']} æ ·æœ¬")
    print(f"   éªŒè¯é›†: {split_info['val_size']} æ ·æœ¬")
    print(f"   æ€»è®¡: {split_info['total_size']} æ ·æœ¬")
    print(f"   ç§å­: {split_info['seed']}")
    print(f"   æ¯”ä¾‹: {split_info['split_ratio']}")

    return True


# ============================================================================
# ä½¿ç”¨ç¤ºä¾‹
# ============================================================================
"""
ä¸ICEdit_contrastiveä¿æŒä¸€è‡´çš„ç¼“å­˜è®¾ç½®ï¼š

æ–¹æ³•1: åœ¨å‘½ä»¤è¡Œè®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆæ¨èï¼‰
    export HF_DATASETS_CACHE="/root/autodl-tmp/.cache"
    export HUGGINGFACE_HUB_CACHE="/root/autodl-tmp/.cache"
    python utils/fixed_split.py --parquet /path/to/data.parquet

æ–¹æ³•2: åœ¨è®­ç»ƒè„šæœ¬ä¸­è®¾ç½®ï¼ˆä¸ICEdit train_joint.shç›¸åŒï¼‰
    #!/bin/bash
    DATA_CACHE="/root/autodl-tmp/.cache"
    export HF_DATASETS_CACHE="$DATA_CACHE"
    export HUGGINGFACE_HUB_CACHE="$DATA_CACHE"
    python scripts/pretrain_decomposition.py

æ–¹æ³•3: åœ¨Pythonä»£ç ä¸­è®¾ç½®
    import os
    os.environ['HF_DATASETS_CACHE'] = '/root/autodl-tmp/.cache'
    from utils.fixed_split import load_split_dataset
    train_ds, val_ds = load_split_dataset('/path/to/data.parquet')

è¿™æ ·ç¡®ä¿ï¼š
1. æ•°æ®é›†ç¼“å­˜ä¸ä¼šé‡å¤ä¸‹è½½
2. é˜¶æ®µ1å’Œé˜¶æ®µ2ä½¿ç”¨ç›¸åŒçš„ç¼“å­˜ä½ç½®
3. èŠ‚çœç£ç›˜ç©ºé—´
"""

if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser(description='åˆ›å»ºå›ºå®šçš„æ•°æ®é›†åˆ’åˆ†')
    parser.add_argument('--parquet', type=str, required=True,
                       help='Parquetæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output_dir', type=str, default='./data',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--split_ratio', type=float, default=0.9,
                       help='è®­ç»ƒé›†æ¯”ä¾‹ (é»˜è®¤0.9)')
    parser.add_argument('--seed', type=int, default=42,
                       help='éšæœºç§å­ (é»˜è®¤42)')
    parser.add_argument('--force', action='store_true',
                       help='å¼ºåˆ¶é‡æ–°åˆ›å»ºåˆ’åˆ†')
    parser.add_argument('--verify', action='store_true',
                       help='éªŒè¯åˆ’åˆ†ä¸€è‡´æ€§')

    args = parser.parse_args()

    # åˆ›å»ºå›ºå®šåˆ’åˆ†
    split_info = create_fixed_split(
        parquet_path=args.parquet,
        split_ratio=args.split_ratio,
        seed=args.seed,
        output_dir=args.output_dir,
        force_recreate=args.force
    )

    print("\n" + "="*60)
    print("ğŸ“‹ åˆ’åˆ†æ‘˜è¦:")
    print("="*60)
    print(f"Parquetè·¯å¾„: {split_info['parquet_path']}")
    print(f"æ€»æ ·æœ¬æ•°: {split_info['total_size']}")
    print(f"è®­ç»ƒé›†: {split_info['train_size']} ({split_info['train_size']/split_info['total_size']*100:.1f}%)")
    print(f"éªŒè¯é›†: {split_info['val_size']} ({split_info['val_size']/split_info['total_size']*100:.1f}%)")
    print(f"éšæœºç§å­: {split_info['seed']}")
    print(f"åˆ’åˆ†æ–‡ä»¶: {args.output_dir}/dataset_split.json")
    print("="*60)

    # éªŒè¯ï¼ˆå¦‚æœè¯·æ±‚ï¼‰
    if args.verify:
        print("\n")
        verify_split_consistency(f"{args.output_dir}/dataset_split.json")

    print("\nâœ… å®Œæˆ!")
    print("\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
    print(f"   åœ¨è®­ç»ƒè„šæœ¬ä¸­å¯¼å…¥: from utils.fixed_split import load_split_dataset")
    print(f"   åŠ è½½æ•°æ®é›†: train_ds, val_ds = load_split_dataset('{args.parquet}')")
